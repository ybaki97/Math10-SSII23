# UC Irvine, Math 10, Summer 2023
___Introduction to programming for data science___

__Introduction:__ 
Welcome to Math 10 at UC Irvine during Summer Session II 2023. This course is
organized into two main components: Exploratory Data Analysis (EDA) and Machine
Learning (ML). We will divide our class time equally between these two topics,
and the class will culminate in students submitting a notebook analyzing a 
dataset (not presented in Math 10) of their choice. 

__Student Learning Outcomes:__ Our section of Math 10 makes use of 
_specifications grading,_ and grades in the class are largely determined by 
demonstrating mastery of the following Student Learning Outcomes (SLOs): 

__EDA Outcomes:__
* P1. Perform boolean indexing on data frames.
    
* P2. Answer the same question using multiple techniques (e.g. 
"determine which penguin species has the longest median bill length" using a
for-loop, graphically, and pandas `groupby`)

* P3. Generate random data frames.

* P4. Select appropriate indexing methods in pandas (e.g. `df.loc` versus `df.iloc`).

* P5. Transform data sets using pandas and NumPy (e.g. `any`, `all`…).

* P6. Work with datetime objects in pandas.

* P7. Work with groupby objects in pandas.

* P8. Pick the appropriate data type for a given computation.

* P9. Deduce key properties and insights of an unfamiliar dataset (e.g. use of `isna`, `value_counts`…)

* P10. Use Altair to produce visualizations of data and convey key aspects.


__ML Outcomes:__
* P11. Determine whether a problem is supervised/unsupervised and regression/classification.

* P12. Perform polynomial regression using scikit-learn (by hand, using "polynomial features", and using the pipeline class).
This includes performing linear regression and interpreting coefficients.

* P13. Use performance measures to determine which polynomial best fits data.

* P14. Perform binary and non-binary classification using logistic regression; interpret confidence.

* P15. Interpret a visualization which shows a decision boundary.

* P16. Understand the risks of overfitting and underfitting, and select appropriate loss functions for given data.

* P17. Use a `train_test_split` to detect overfitting.

* P18. Perform a few iterations of the K-means clustering algorithm by hand.

* P19. Use scikit-learn and preprocessing techniques like StandardScaler to perform K-means clustering.

* P20. Maintain a well-organized Deepnote "Problem Notebook".









```{tableofcontents}
```
